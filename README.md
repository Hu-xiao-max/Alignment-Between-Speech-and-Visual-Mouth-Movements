# Alignment-Between-Speech-and-Visual-Mouth-Movements

1.Dataset download

https://www.kaggle.com/datasets/jedidiahangekouakou/grid-corpus-dataset-for-training-lipnet/data

unzip in follow path：Alignment-Between-Speech-and-Visual-Mouth-Movements/data/


## Project Structure

| Path | Purpose |
| --- | --- |
| `data/` | Root directory for the GRID corpus. The structure should follow `data/s{speaker_id}/{video,align}` or contain paired video/text files for each speaker. |
| `checkpoints/` | Saved TensorFlow/Keras checkpoints used by the original LipNet training pipeline. |
| `logs/` | TensorBoard logs and training artifacts generated by the TensorFlow workflow. |
| `LipNet/` | Upstream LipNet reference implementation kept for comparison or reuse of preprocessing utilities. |
| `data.zip` | Optional archive containing the GRID dataset; unzip it into `data/` before running any scripts. |
| `shape_predictor_68_face_landmarks.dat` | Pretrained dlib facial landmark model used for accurate mouth region extraction. |
| `main.py` | PyTorch entry point that checks the dataset, splits speakers, and launches the training/validation loop. |
| `train.py` | TensorFlow implementation with CLI arguments to train, test, or run inference on the GRID dataset. |
| `main_backup.py` | Earlier version of the PyTorch pipeline retained for reference. |
| `model.py` | Defines the PyTorch LipNet architecture (3D CNN + Bi-GRU + CTC head). |
| `dataset.py` | Implements the `GridDataset`, alignment parsing, vocabulary utilities, and custom `collate_fn`. |
| `trainer.py` | Handles PyTorch training/validation loops, loss logging, and checkpoint saving. |
| `utils.py` | Helper functions for dataset inspection, decoding CTC outputs, evaluation, and creating dummy alignments. |
| `predict.py` | Loads a trained PyTorch checkpoint and runs evaluation on the held-out test speakers. |
| `run_train.sh` | Convenience shell script that activates the conda environment, sets CUDA/cuDNN paths, and invokes `train.py`. |
| `requirements.txt` | Python dependencies required for both TensorFlow and PyTorch workflows. |
| `training_history.png` | Example loss curves captured from a TensorFlow training run for quick diagnostics. |
| `grid-corpus-dataset-for-training-lipnet.ipynb` | Notebook adaptation of the dataset and training pipeline for exploratory experiments. |
| `lipnet_final.pth` | PyTorch model weights produced by `trainer.py`. |
| `README.md` | Project overview, setup instructions, and file descriptions (this document). |

## 安装依赖
  pip install -r requirements.txt

  ### 下载 dlib 人脸特征点检测模型（optional）
  wget https://github.com/italojs/facial-landmarks-recognition/raw/master/shape_predictor_68_face_landmarks.dat


### 训练模型
./run_train.sh --mode train --epochs 100 --batch_size 8

### 测试模型
./run_train.sh  python train.py --mode test --checkpoint checkpoints/lipnet_best.keras

  ### 单视频推理
  ./run_train.sh python train.py --mode inference --checkpoint checkpoints/lipnet_best.keras --video ./data/s1_processed/bbaf2n.mpg

## Misalignment Detection Pipeline

### misalignment Detection Train

if on NEU HPC, run:

```bash
sbatch run_train_misalignment.sh
```

else:

install PyTorch

```bash
pip install librosa soundfile scikit-learn matplotlib opencv-python
```

```bash
python misalignment_detection_train.py \
  --data_path ./data \
  --checkpoint lipnet_final.pth \
  --detector_checkpoint misalignment_detector.pth \
  --max_samples 100 \
  --epochs 10 \
  --max_shift_frames 20 \
  --log_dir logs
```

| Parameter | Description |
|-----------|-------------|
| `--data_path ./data` | Root directory containing the GRID corpus dataset |
| `--speakers s1_processed` | Speaker folder(s) to use for training (e.g., s1_processed, s2_processed) |
| `--checkpoint lipnet_final.pth` | Path to the pretrained LipNet model weights (used for visual feature extraction) |
| `--detector_checkpoint misalignment_detector.pth` | Output path to save the trained misalignment detector model |
| `--max_samples 200` | Maximum number of video samples to use for training (limits dataset size for faster training) |
| `--epochs 20` | Number of training epochs |
| `--max_shift_frames 15` | Maximum audio shift in frames for generating misaligned (negative) samples. At 25fps, 15 frames = 0.6 seconds |
| `--save_roc logs/roc.png` | Output path to save the ROC curve plot after evaluation |To run code, enable code execution and file creation in Settings > Capabilities.


### Demo Generation

```bash
python misalignment_detection_demo.py \
 --detector_checkpoint misalignment_detector_all_30epoch_20shift_512hidden.pth \
 --save_demo_dir demos/misalignment_20251129_183628_demos \
 --min_shift 5 \
 --max_shift 20 \
 --demo_include_audio
```

| Parameter | Description |
|-----------|-------------|
| `--data_path ./data` | Root directory containing the GRID corpus dataset |
| `--speakers s1_processed` | Speaker folder(s) to use for selecting demo video (randomly select one video from the folder) |
| `--checkpoint lipnet_final.pth` | Path to the pretrained LipNet model weights (used for visual feature extraction) |
| `--detector_checkpoint misalignment_detector_200_20_15.pth` | Path to the trained misalignment detector model |
| `--save_demo_dir demos` | Output directory to save the generated demo videos |
| `--demo_include_audio` | Include audio tracks in the output videos |
| `--demo_scale 2.0` | Upscale factor for video resolution (2.0 = double the size) |
| `--demo_audio_sample_rate 44100` | Audio sample rate in Hz for the output videos (44100 = CD quality) | -->

